{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "669933c4-1067-425f-a5ca-9e4b1b76e54a",
   "metadata": {},
   "source": [
    "## Config and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d2ba7d9-26b3-4380-9d5b-3a905782a23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./ckpt/FFHQ_256.pt\"\n",
    "device = \"cuda\"\n",
    "image_size = 256\n",
    "image_width, image_height, rgb_channel, rgba_channel = 256, 256, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "851b5e16-6be7-42f4-bff3-e921b2808e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PublicF\\anaconda3\\envs\\stylegan3\\lib\\site-packages\\diffusers\\utils\\outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AddedDllDirectory('C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.7\\\\bin')>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.transforms import functional\n",
    "import os\n",
    "from models.generator import *\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from array import array\n",
    "from draggan import DragGAN\n",
    "import lpips\n",
    "from einops import rearrange\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dift_sd import SDFeaturizer\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "os.add_dll_directory(\"C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.7\\\\bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff67578-68a2-4cad-9b38-fa0ff9bd3ea8",
   "metadata": {},
   "source": [
    "## Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14ee18f1-8f09-4cfb-b956-1ac7548f6c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PublicF\\anaconda3\\envs\\stylegan3\\lib\\site-packages\\torch\\functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3550.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading checkpoint from ./ckpt/LSUNChurch_256.pt\n",
      "input size:  256\n",
      "change generator to 256 weights\n",
      "loading checkpoint successed!\n"
     ]
    }
   ],
   "source": [
    "model = DragGAN(device)\n",
    "model.load_ckpt(model_path)\n",
    "\n",
    "# raw data\n",
    "raw_data_size = image_width * image_height * rgba_channel\n",
    "raw_data = array('f', [1] * raw_data_size)\n",
    "origin_image = array('f', [1] * raw_data_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b61c4e4-0e41-4300-9691-e0809823fb22",
   "metadata": {},
   "source": [
    "## Generate points and masks\n",
    "\n",
    "we don't neeed to \"draw\" any more so just return the point array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6ef9bd1-178d-47de-97c6-86229cdd5e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 0, overlay: 0, layer: 0\n"
     ]
    }
   ],
   "source": [
    "# points parameters\n",
    "points = [np.array([120, 150]), np.array([105, 150])]\n",
    "ptn = 2\n",
    "\n",
    "# mask parameters\n",
    "mask = np.ones([256, 256], np.uint8)\n",
    "\n",
    "# seed\n",
    "seeds = [156,251,653,1896,2534]\n",
    "\n",
    "# overlay\n",
    "overlays = [0, 1]\n",
    "\n",
    "# record class\n",
    "# overlay = [0, 1]\n",
    "# slayer = [6, 7]\n",
    "# sll = range(1,9)\n",
    "class Record:\n",
    "    def __init__(self, seed, overlay, layer, sll):\n",
    "        self.seed = seed\n",
    "        self.overlay = overlay\n",
    "        self.layer = layer\n",
    "        self.sll = sll\n",
    "    def __str__(self):\n",
    "        return \"seed: \" + str(self.seed) + \", overlay: \" + str(self.overlay) + \", layer: \" + str(self.layer) + \", sll: \" + str(self.sll)\n",
    "\n",
    "# records = [Record(0,0,0,0)]\n",
    "records = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cb95c0-c38a-4ca9-a8d8-c1594a3d73da",
   "metadata": {},
   "source": [
    "## Step & Score\n",
    "\n",
    "1. generate image with different seed\n",
    "2. while dragging, do thresholding and update the mask\n",
    "3. calculate scores at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "600e8280-1404-4360-96b4-19be5f10fa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z:  torch.Size([1, 512])\n",
      "p0:  [(120, 150)]\n",
      "tensor(7.0829, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor distance:  tensor([15.])\n",
      "tensor(7.4676, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor distance:  tensor([15.])\n",
      "tensor(7.4715, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor distance:  tensor([15.])\n",
      "tensor(7.4484, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor distance:  tensor([15.])\n",
      "tensor(7.4060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor distance:  tensor([15.])\n",
      "tensor(7.3771, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor distance:  tensor([15.])\n",
      "tensor(7.3310, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor distance:  tensor([15.])\n",
      "tensor(7.3043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor distance:  tensor([15.])\n",
      "tensor(7.3122, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor distance:  tensor([15.])\n",
      "tensor(7.3258, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor distance:  tensor([15.])\n",
      "tensor(7.3308, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor distance:  tensor([15.])\n",
      "tensor(7.3482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor distance:  tensor([15.])\n",
      "tensor(7.3635, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor distance:  tensor([15.])\n",
      "tensor(7.3789, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# dragging\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# we need to automate different layer selection \u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (dragging):\n\u001b[1;32m---> 25\u001b[0m     status, ret, sfeatures, simage \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverlay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status:\n\u001b[0;32m     27\u001b[0m         points, image \u001b[38;5;241m=\u001b[39m ret\n",
      "File \u001b[1;32m~\\Documents\\workspace\\StyleSwin\\draggan.py:369\u001b[0m, in \u001b[0;36mDragGAN.step\u001b[1;34m(self, points, mask, overlay, selection)\u001b[0m\n\u001b[0;32m    365\u001b[0m features \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_index\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    366\u001b[0m \u001b[38;5;66;03m# features = features[self.layer_index+1]\u001b[39;00m\n\u001b[0;32m    368\u001b[0m store_image \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2.8\u001b[39m)) \u001b[38;5;241m*\u001b[39m \\\n\u001b[1;32m--> 369\u001b[0m     \u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m    370\u001b[0m image \u001b[38;5;241m=\u001b[39m store_image\u001b[38;5;241m.\u001b[39mclip(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp0)):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# functions\n",
    "def update_image(new_image):\n",
    "    for i in range(0, 256*256):\n",
    "        rd_base, im_base = i * rgba_channel, i * rgb_channel\n",
    "        raw_data[rd_base:rd_base + rgb_channel] = array('f', new_image[im_base:im_base + rgb_channel])\n",
    "\n",
    "\n",
    "for seed in seeds:\n",
    "    for overlay in overlays:\n",
    "        # generate image and origin data\n",
    "        image = model.generate_image(seed)\n",
    "        update_image(image)\n",
    "        for i in range(0, 256*256):\n",
    "            rd_base, im_base = i * rgba_channel, i * rgb_channel\n",
    "            origin_image[rd_base:rd_base + rgb_channel] = array('f', image[im_base:im_base + rgb_channel])\n",
    "    \n",
    "    \n",
    "        # dragging parameters\n",
    "        step = 0\n",
    "        dragging = True\n",
    "        \n",
    "        # dragging\n",
    "        # we need to automate different layer selection \n",
    "        while (dragging):\n",
    "            status, ret, sfeatures, simage = model.step(points, mask, overlay)\n",
    "            if status:\n",
    "                points, image = ret\n",
    "            else:\n",
    "                dragging = False\n",
    "                continue\n",
    "            update_image(image)\n",
    "            # update_mask(mask)\n",
    "            step += 1\n",
    "            \n",
    "        # call calculate score function after dragging\n",
    "        lpips = model.lpip_score(origin_image, raw_data)\n",
    "        mds = model.mean_distance_score(origin_image, raw_data, points)\n",
    "        print(\"lpips:\", lpips)\n",
    "        print(\"mds:\", mds)\n",
    "        # fixed layer to 6 for testing\n",
    "        # same with draggan.py line 246\n",
    "        records.append(Record(seed, overlay, 6, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca050a2-fa22-4eb5-ad44-cfd035dce2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,55))\n",
    "\n",
    "im_w, im_h = 256, 256\n",
    "points = [np.array([120, 150]), np.array([90, 150])]\n",
    "ptn = 2\n",
    "\n",
    "extract_layers = [6,7]\n",
    "\n",
    "for el in extract_layers:\n",
    "    d = features[0][el].size(dim=1)\n",
    "    d2 = features[0][el].size(dim=2)\n",
    "    feature = features[0][el].transpose(-1,-2).reshape(1, d2, int(math.sqrt(d)), int(math.sqrt(d)))\n",
    "\n",
    "    img = feature[0]\n",
    "    if el == 6:\n",
    "        sixthlayer = img.cpu().numpy()\n",
    "    if el == 7:\n",
    "        seventhlayer = img.cpu().numpy()\n",
    "\n",
    "    for k in range(1,9):\n",
    "        \n",
    "        temp = np.zeros((im_w, im_h))\n",
    "\n",
    "        temp = img[k] - img[k].min()\n",
    "        temp = temp / temp.max()\n",
    "        temp = temp * 255\n",
    "        temp = temp.cpu().numpy().astype(np.uint8)\n",
    "        \n",
    "        print(temp.max(), temp.min())\n",
    "\n",
    "        blur = cv2.GaussianBlur(temp,(5,5),0)\n",
    "        ret3, thres3 = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "        T = int(ret3)\n",
    "        P = temp[insert_order[0][1], insert_order[0][0]]\n",
    "\n",
    "\n",
    "# plt.subplot(17,3,cnt)\n",
    "# img = (1/(2*2.8)) * image[0].permute(1,2,0).cpu().numpy() + 0.5\n",
    "# img = functional.to_pil_image(img , mode=\"RGB\")\n",
    "# plt.imshow(img)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
